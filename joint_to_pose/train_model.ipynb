{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import keras\n",
    "import os\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load the data from csv we get before in the stage preprocess the data\n",
    "\n",
    "1. read csv \n",
    "2. split out the target and the class name\n",
    "3. drop the data we don't need in training model\n",
    "\n",
    "### return data, target, class name , original data from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_from_csv(csv_path: str):\n",
    "    ori_dataframe = pd.read_csv(csv_path)\n",
    "    dataframe = ori_dataframe.copy()\n",
    "    \n",
    "    \n",
    "    class_no = dataframe[\"class_no\"] # get the labeled data of each posture data (1 dimension of int)\n",
    "    class_name = dataframe[\"class_name\"]# get the labeled data of each posture data (1 dimension of string)\n",
    "    dataframe.drop(columns=['file_name',\"class_no\",\"class_name\"], inplace=True) # drop the useles data for training\n",
    "    \n",
    "    \n",
    "    x = dataframe\n",
    "    y = keras.utils.to_categorical(class_no)\n",
    "    print(y)\n",
    "    class_name = class_name.unique()\n",
    "    \n",
    "    return x,y,class_name,ori_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Desktop\\sophomore_2nd_semester\\Artificial_Intelligence\\final_project\\Human-pose-estimation\\code\\train_model\\test_data2.csv\n",
      "[[1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "train_file = os.getcwd()+\"\\\\test_data2.csv\"\n",
    "print(train_file)\n",
    "x_train,y_train,class_names,_ = load_data_from_csv(train_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nose_x</th>\n",
       "      <th>nose_y</th>\n",
       "      <th>nose_score</th>\n",
       "      <th>left_eye_x</th>\n",
       "      <th>left_eye_y</th>\n",
       "      <th>left_eye_score</th>\n",
       "      <th>right_eye_x</th>\n",
       "      <th>right_eye_y</th>\n",
       "      <th>right_eye_score</th>\n",
       "      <th>left_ear_x</th>\n",
       "      <th>...</th>\n",
       "      <th>left_knee_score</th>\n",
       "      <th>right_knee_x</th>\n",
       "      <th>right_knee_y</th>\n",
       "      <th>right_knee_score</th>\n",
       "      <th>left_ankle_x</th>\n",
       "      <th>left_ankle_y</th>\n",
       "      <th>left_ankle_score</th>\n",
       "      <th>right_ankle_x</th>\n",
       "      <th>right_ankle_y</th>\n",
       "      <th>right_ankle_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1601.0</td>\n",
       "      <td>1721.0</td>\n",
       "      <td>0.619423</td>\n",
       "      <td>1526.0</td>\n",
       "      <td>1690.0</td>\n",
       "      <td>0.537892</td>\n",
       "      <td>1513.0</td>\n",
       "      <td>1697.0</td>\n",
       "      <td>0.559045</td>\n",
       "      <td>1424.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.428641</td>\n",
       "      <td>2779.0</td>\n",
       "      <td>1910.0</td>\n",
       "      <td>0.217417</td>\n",
       "      <td>2906.0</td>\n",
       "      <td>2195.0</td>\n",
       "      <td>0.178153</td>\n",
       "      <td>2880.0</td>\n",
       "      <td>2114.0</td>\n",
       "      <td>0.291468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1545.0</td>\n",
       "      <td>1540.0</td>\n",
       "      <td>0.550073</td>\n",
       "      <td>1459.0</td>\n",
       "      <td>1497.0</td>\n",
       "      <td>0.420967</td>\n",
       "      <td>1445.0</td>\n",
       "      <td>1510.0</td>\n",
       "      <td>0.539037</td>\n",
       "      <td>1353.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.425792</td>\n",
       "      <td>2069.0</td>\n",
       "      <td>1829.0</td>\n",
       "      <td>0.396186</td>\n",
       "      <td>2923.0</td>\n",
       "      <td>1934.0</td>\n",
       "      <td>0.244803</td>\n",
       "      <td>2799.0</td>\n",
       "      <td>1927.0</td>\n",
       "      <td>0.320831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1081.0</td>\n",
       "      <td>1174.0</td>\n",
       "      <td>0.508293</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>1094.0</td>\n",
       "      <td>0.482821</td>\n",
       "      <td>999.0</td>\n",
       "      <td>1117.0</td>\n",
       "      <td>0.306868</td>\n",
       "      <td>1019.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.068202</td>\n",
       "      <td>2066.0</td>\n",
       "      <td>1526.0</td>\n",
       "      <td>0.154774</td>\n",
       "      <td>2655.0</td>\n",
       "      <td>1620.0</td>\n",
       "      <td>0.307416</td>\n",
       "      <td>2572.0</td>\n",
       "      <td>1523.0</td>\n",
       "      <td>0.147803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1638.0</td>\n",
       "      <td>1501.0</td>\n",
       "      <td>0.541167</td>\n",
       "      <td>1562.0</td>\n",
       "      <td>1430.0</td>\n",
       "      <td>0.506505</td>\n",
       "      <td>1547.0</td>\n",
       "      <td>1448.0</td>\n",
       "      <td>0.333264</td>\n",
       "      <td>1561.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.250234</td>\n",
       "      <td>2069.0</td>\n",
       "      <td>1662.0</td>\n",
       "      <td>0.239503</td>\n",
       "      <td>2914.0</td>\n",
       "      <td>1781.0</td>\n",
       "      <td>0.191041</td>\n",
       "      <td>2894.0</td>\n",
       "      <td>1879.0</td>\n",
       "      <td>0.268571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1316.0</td>\n",
       "      <td>1667.0</td>\n",
       "      <td>0.473499</td>\n",
       "      <td>1253.0</td>\n",
       "      <td>1642.0</td>\n",
       "      <td>0.681309</td>\n",
       "      <td>1230.0</td>\n",
       "      <td>1635.0</td>\n",
       "      <td>0.753154</td>\n",
       "      <td>1205.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.549620</td>\n",
       "      <td>1923.0</td>\n",
       "      <td>1825.0</td>\n",
       "      <td>0.404312</td>\n",
       "      <td>3216.0</td>\n",
       "      <td>1570.0</td>\n",
       "      <td>0.146073</td>\n",
       "      <td>2545.0</td>\n",
       "      <td>1788.0</td>\n",
       "      <td>0.245440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>1357.0</td>\n",
       "      <td>1794.0</td>\n",
       "      <td>0.474139</td>\n",
       "      <td>1322.0</td>\n",
       "      <td>1784.0</td>\n",
       "      <td>0.484525</td>\n",
       "      <td>1312.0</td>\n",
       "      <td>1839.0</td>\n",
       "      <td>0.603766</td>\n",
       "      <td>1337.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.368134</td>\n",
       "      <td>2831.0</td>\n",
       "      <td>1161.0</td>\n",
       "      <td>0.647797</td>\n",
       "      <td>3484.0</td>\n",
       "      <td>1336.0</td>\n",
       "      <td>0.417314</td>\n",
       "      <td>3503.0</td>\n",
       "      <td>1379.0</td>\n",
       "      <td>0.451009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>1275.0</td>\n",
       "      <td>1591.0</td>\n",
       "      <td>0.292113</td>\n",
       "      <td>1238.0</td>\n",
       "      <td>1591.0</td>\n",
       "      <td>0.375394</td>\n",
       "      <td>1240.0</td>\n",
       "      <td>1624.0</td>\n",
       "      <td>0.402719</td>\n",
       "      <td>1239.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.348758</td>\n",
       "      <td>2609.0</td>\n",
       "      <td>1012.0</td>\n",
       "      <td>0.175212</td>\n",
       "      <td>2533.0</td>\n",
       "      <td>1371.0</td>\n",
       "      <td>0.172673</td>\n",
       "      <td>2771.0</td>\n",
       "      <td>1310.0</td>\n",
       "      <td>0.211996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>1555.0</td>\n",
       "      <td>1294.0</td>\n",
       "      <td>0.491425</td>\n",
       "      <td>1502.0</td>\n",
       "      <td>1290.0</td>\n",
       "      <td>0.652936</td>\n",
       "      <td>1522.0</td>\n",
       "      <td>1306.0</td>\n",
       "      <td>0.492146</td>\n",
       "      <td>1442.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.519203</td>\n",
       "      <td>2862.0</td>\n",
       "      <td>992.0</td>\n",
       "      <td>0.707952</td>\n",
       "      <td>3457.0</td>\n",
       "      <td>1143.0</td>\n",
       "      <td>0.481670</td>\n",
       "      <td>3450.0</td>\n",
       "      <td>1141.0</td>\n",
       "      <td>0.402422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>2159.0</td>\n",
       "      <td>1979.0</td>\n",
       "      <td>0.065284</td>\n",
       "      <td>2062.0</td>\n",
       "      <td>2092.0</td>\n",
       "      <td>0.056015</td>\n",
       "      <td>2089.0</td>\n",
       "      <td>2028.0</td>\n",
       "      <td>0.043993</td>\n",
       "      <td>2657.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.382332</td>\n",
       "      <td>2772.0</td>\n",
       "      <td>1157.0</td>\n",
       "      <td>0.491889</td>\n",
       "      <td>2518.0</td>\n",
       "      <td>1494.0</td>\n",
       "      <td>0.059322</td>\n",
       "      <td>2852.0</td>\n",
       "      <td>1708.0</td>\n",
       "      <td>0.184458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>1201.0</td>\n",
       "      <td>1427.0</td>\n",
       "      <td>0.462897</td>\n",
       "      <td>1167.0</td>\n",
       "      <td>1407.0</td>\n",
       "      <td>0.599190</td>\n",
       "      <td>1144.0</td>\n",
       "      <td>1441.0</td>\n",
       "      <td>0.441581</td>\n",
       "      <td>1198.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.674192</td>\n",
       "      <td>2843.0</td>\n",
       "      <td>893.0</td>\n",
       "      <td>0.778255</td>\n",
       "      <td>3177.0</td>\n",
       "      <td>1279.0</td>\n",
       "      <td>0.258643</td>\n",
       "      <td>3695.0</td>\n",
       "      <td>1048.0</td>\n",
       "      <td>0.219055</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>118 rows Ã— 51 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     nose_x  nose_y  nose_score  left_eye_x  left_eye_y  left_eye_score  \\\n",
       "0    1601.0  1721.0    0.619423      1526.0      1690.0        0.537892   \n",
       "1    1545.0  1540.0    0.550073      1459.0      1497.0        0.420967   \n",
       "2    1081.0  1174.0    0.508293      1001.0      1094.0        0.482821   \n",
       "3    1638.0  1501.0    0.541167      1562.0      1430.0        0.506505   \n",
       "4    1316.0  1667.0    0.473499      1253.0      1642.0        0.681309   \n",
       "..      ...     ...         ...         ...         ...             ...   \n",
       "113  1357.0  1794.0    0.474139      1322.0      1784.0        0.484525   \n",
       "114  1275.0  1591.0    0.292113      1238.0      1591.0        0.375394   \n",
       "115  1555.0  1294.0    0.491425      1502.0      1290.0        0.652936   \n",
       "116  2159.0  1979.0    0.065284      2062.0      2092.0        0.056015   \n",
       "117  1201.0  1427.0    0.462897      1167.0      1407.0        0.599190   \n",
       "\n",
       "     right_eye_x  right_eye_y  right_eye_score  left_ear_x  ...  \\\n",
       "0         1513.0       1697.0         0.559045      1424.0  ...   \n",
       "1         1445.0       1510.0         0.539037      1353.0  ...   \n",
       "2          999.0       1117.0         0.306868      1019.0  ...   \n",
       "3         1547.0       1448.0         0.333264      1561.0  ...   \n",
       "4         1230.0       1635.0         0.753154      1205.0  ...   \n",
       "..           ...          ...              ...         ...  ...   \n",
       "113       1312.0       1839.0         0.603766      1337.0  ...   \n",
       "114       1240.0       1624.0         0.402719      1239.0  ...   \n",
       "115       1522.0       1306.0         0.492146      1442.0  ...   \n",
       "116       2089.0       2028.0         0.043993      2657.0  ...   \n",
       "117       1144.0       1441.0         0.441581      1198.0  ...   \n",
       "\n",
       "     left_knee_score  right_knee_x  right_knee_y  right_knee_score  \\\n",
       "0           0.428641        2779.0        1910.0          0.217417   \n",
       "1           0.425792        2069.0        1829.0          0.396186   \n",
       "2           0.068202        2066.0        1526.0          0.154774   \n",
       "3           0.250234        2069.0        1662.0          0.239503   \n",
       "4           0.549620        1923.0        1825.0          0.404312   \n",
       "..               ...           ...           ...               ...   \n",
       "113         0.368134        2831.0        1161.0          0.647797   \n",
       "114         0.348758        2609.0        1012.0          0.175212   \n",
       "115         0.519203        2862.0         992.0          0.707952   \n",
       "116         0.382332        2772.0        1157.0          0.491889   \n",
       "117         0.674192        2843.0         893.0          0.778255   \n",
       "\n",
       "     left_ankle_x  left_ankle_y  left_ankle_score  right_ankle_x  \\\n",
       "0          2906.0        2195.0          0.178153         2880.0   \n",
       "1          2923.0        1934.0          0.244803         2799.0   \n",
       "2          2655.0        1620.0          0.307416         2572.0   \n",
       "3          2914.0        1781.0          0.191041         2894.0   \n",
       "4          3216.0        1570.0          0.146073         2545.0   \n",
       "..            ...           ...               ...            ...   \n",
       "113        3484.0        1336.0          0.417314         3503.0   \n",
       "114        2533.0        1371.0          0.172673         2771.0   \n",
       "115        3457.0        1143.0          0.481670         3450.0   \n",
       "116        2518.0        1494.0          0.059322         2852.0   \n",
       "117        3177.0        1279.0          0.258643         3695.0   \n",
       "\n",
       "     right_ankle_y  right_ankle_score  \n",
       "0           2114.0           0.291468  \n",
       "1           1927.0           0.320831  \n",
       "2           1523.0           0.147803  \n",
       "3           1879.0           0.268571  \n",
       "4           1788.0           0.245440  \n",
       "..             ...                ...  \n",
       "113         1379.0           0.451009  \n",
       "114         1310.0           0.211996  \n",
       "115         1141.0           0.402422  \n",
       "116         1708.0           0.184458  \n",
       "117         1048.0           0.219055  \n",
       "\n",
       "[118 rows x 51 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "test_file = os.getcwd()+\"\\\\test_data2.csv\"\n",
    "x_test,y_test,_,test_dataframe= load_data_from_csv(test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_center_point(landmarks, left_point, right_point):\n",
    "    \"\"\"Calculates the center point of the two given landmarks.\"\"\"\n",
    "\n",
    "    left = tf.gather(landmarks, left_point, axis=1)\n",
    "    right = tf.gather(landmarks, right_point, axis=1)\n",
    "    #get landmarks[left_bodypart.value]\n",
    "    center = (left+right)/2\n",
    "    return center\n",
    "  \n",
    "def get_pose_size(landmarks, torso_size_multiplier=2.5):\n",
    "  \"\"\"Calculates pose size.\n",
    "\n",
    "  It is the maximum of two values:\n",
    "    * Torso size multiplied by `torso_size_multiplier`\n",
    "    * Maximum distance from pose center to any pose landmark\n",
    "  \"\"\"\n",
    "  # Hips center\n",
    "  hips_center = get_center_point(landmarks, \n",
    "                                 11, #left hips is 11th data in csv\n",
    "                                 12  #right hips is 12th data in csv\n",
    "                                 )\n",
    "\n",
    "  # Shoulders center\n",
    "  shoulders_center = get_center_point(landmarks, \n",
    "                                      5, #left shoulders is 5th data in csv\n",
    "                                      6  #right shoulders is 6th data in csv\n",
    "                                      )\n",
    "\n",
    "  # we defined the torso size as the length of shouder to hip\n",
    "  torso_size = tf.linalg.norm(shoulders_center - hips_center)\n",
    "\n",
    "  # when we got the center point we expand the dimension of center point\n",
    "  pose_center_new = get_center_point(landmarks, \n",
    "                                     11, #left hips is 11th data in csv\n",
    "                                     12  #right hips is 12th data in csv\n",
    "                                     )\n",
    "  # when we got the center point we expand the dimension of center point \n",
    "  pose_center_new = tf.expand_dims(pose_center_new, axis=1)\n",
    "  \n",
    "  #and braodcast to the size of the landmarks\n",
    "  pose_center_new = tf.broadcast_to(pose_center_new,\n",
    "                                    [tf.size(landmarks) // (17*2), 17, 2])\n",
    "\n",
    "  # we get the 0-index of Dist to pose center\n",
    "  d = tf.gather(landmarks - pose_center_new, 0, axis=0,\n",
    "                name=\"dist_to_pose_center\")\n",
    "  \n",
    "  # Max dist to pose center\n",
    "  max_dist = tf.reduce_max(tf.linalg.norm(d, axis=0))\n",
    "\n",
    "  # Normalize scale\n",
    "  pose_size = tf.maximum(torso_size * torso_size_multiplier, max_dist)\n",
    "\n",
    "  return pose_size\n",
    "\n",
    "\n",
    "def normalize_pose_inputs(reshaped_inputs):\n",
    "  \"\"\"Normalizes the landmarks translation by moving the pose center to (0,0) and\n",
    "  scaling it to a constant pose size.\n",
    "  \"\"\"\n",
    "  # we only need the x,y coordinate so the input only x,y without score\n",
    "  landmarks = reshaped_inputs[:, :, :2]\n",
    "  \n",
    "  #we defined the center of posture is the middle of hip\n",
    "  pose_center = get_center_point(landmarks, \n",
    "                                 11, #left hips is 11th data in csv\n",
    "                                 12  #right hips is 12th data in csv\n",
    "                                 )\n",
    "  # when we got the center point we expand the dimension of center point \n",
    "  pose_center = tf.expand_dims(pose_center, axis=1)\n",
    "  #and braodcast to the size of the landmarks\n",
    "  pose_center = tf.broadcast_to(pose_center, \n",
    "                                [tf.size(landmarks) // (17*2), 17, 2])\n",
    "  #subtraction to get the relative coordinate\n",
    "  landmarks = landmarks - pose_center\n",
    "\n",
    "  # Scale the landmarks to a constant pose size\n",
    "  pose_size = get_pose_size(landmarks)\n",
    "  landmarks /= pose_size\n",
    "\n",
    "  return landmarks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the nueral network model correresponding to the input data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_7 (InputLayer)           [(None, 51)]         0           []                               \n",
      "                                                                                                  \n",
      " reshape_6 (Reshape)            (None, 17, 3)        0           ['input_7[0][0]']                \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_6 (Sl  (None, 17, 2)       0           ['reshape_6[0][0]']              \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " tf.compat.v1.gather_54 (TFOpLa  (None, 2)           0           ['tf.__operators__.getitem_6[0][0\n",
      " mbda)                                                           ]']                              \n",
      "                                                                                                  \n",
      " tf.compat.v1.gather_55 (TFOpLa  (None, 2)           0           ['tf.__operators__.getitem_6[0][0\n",
      " mbda)                                                           ]']                              \n",
      "                                                                                                  \n",
      " tf.__operators__.add_24 (TFOpL  (None, 2)           0           ['tf.compat.v1.gather_54[0][0]', \n",
      " ambda)                                                           'tf.compat.v1.gather_55[0][0]'] \n",
      "                                                                                                  \n",
      " tf.math.truediv_30 (TFOpLambda  (None, 2)           0           ['tf.__operators__.add_24[0][0]']\n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.compat.v1.size_12 (TFOpLamb  ()                  0           ['tf.__operators__.getitem_6[0][0\n",
      " da)                                                             ]']                              \n",
      "                                                                                                  \n",
      " tf.expand_dims_12 (TFOpLambda)  (None, 1, 2)        0           ['tf.math.truediv_30[0][0]']     \n",
      "                                                                                                  \n",
      " tf.compat.v1.floor_div_12 (TFO  ()                  0           ['tf.compat.v1.size_12[0][0]']   \n",
      " pLambda)                                                                                         \n",
      "                                                                                                  \n",
      " tf.broadcast_to_12 (TFOpLambda  (None, 17, 2)       0           ['tf.expand_dims_12[0][0]',      \n",
      " )                                                                'tf.compat.v1.floor_div_12[0][0]\n",
      "                                                                 ']                               \n",
      "                                                                                                  \n",
      " tf.math.subtract_18 (TFOpLambd  (None, 17, 2)       0           ['tf.__operators__.getitem_6[0][0\n",
      " a)                                                              ]',                              \n",
      "                                                                  'tf.broadcast_to_12[0][0]']     \n",
      "                                                                                                  \n",
      " tf.compat.v1.gather_60 (TFOpLa  (None, 2)           0           ['tf.math.subtract_18[0][0]']    \n",
      " mbda)                                                                                            \n",
      "                                                                                                  \n",
      " tf.compat.v1.gather_61 (TFOpLa  (None, 2)           0           ['tf.math.subtract_18[0][0]']    \n",
      " mbda)                                                                                            \n",
      "                                                                                                  \n",
      " tf.__operators__.add_27 (TFOpL  (None, 2)           0           ['tf.compat.v1.gather_60[0][0]', \n",
      " ambda)                                                           'tf.compat.v1.gather_61[0][0]'] \n",
      "                                                                                                  \n",
      " tf.math.truediv_33 (TFOpLambda  (None, 2)           0           ['tf.__operators__.add_27[0][0]']\n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.compat.v1.size_13 (TFOpLamb  ()                  0           ['tf.math.subtract_18[0][0]']    \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.compat.v1.gather_58 (TFOpLa  (None, 2)           0           ['tf.math.subtract_18[0][0]']    \n",
      " mbda)                                                                                            \n",
      "                                                                                                  \n",
      " tf.compat.v1.gather_59 (TFOpLa  (None, 2)           0           ['tf.math.subtract_18[0][0]']    \n",
      " mbda)                                                                                            \n",
      "                                                                                                  \n",
      " tf.compat.v1.gather_56 (TFOpLa  (None, 2)           0           ['tf.math.subtract_18[0][0]']    \n",
      " mbda)                                                                                            \n",
      "                                                                                                  \n",
      " tf.compat.v1.gather_57 (TFOpLa  (None, 2)           0           ['tf.math.subtract_18[0][0]']    \n",
      " mbda)                                                                                            \n",
      "                                                                                                  \n",
      " tf.expand_dims_13 (TFOpLambda)  (None, 1, 2)        0           ['tf.math.truediv_33[0][0]']     \n",
      "                                                                                                  \n",
      " tf.compat.v1.floor_div_13 (TFO  ()                  0           ['tf.compat.v1.size_13[0][0]']   \n",
      " pLambda)                                                                                         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_26 (TFOpL  (None, 2)           0           ['tf.compat.v1.gather_58[0][0]', \n",
      " ambda)                                                           'tf.compat.v1.gather_59[0][0]'] \n",
      "                                                                                                  \n",
      " tf.__operators__.add_25 (TFOpL  (None, 2)           0           ['tf.compat.v1.gather_56[0][0]', \n",
      " ambda)                                                           'tf.compat.v1.gather_57[0][0]'] \n",
      "                                                                                                  \n",
      " tf.broadcast_to_13 (TFOpLambda  (None, 17, 2)       0           ['tf.expand_dims_13[0][0]',      \n",
      " )                                                                'tf.compat.v1.floor_div_13[0][0]\n",
      "                                                                 ']                               \n",
      "                                                                                                  \n",
      " tf.math.truediv_32 (TFOpLambda  (None, 2)           0           ['tf.__operators__.add_26[0][0]']\n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.truediv_31 (TFOpLambda  (None, 2)           0           ['tf.__operators__.add_25[0][0]']\n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.subtract_20 (TFOpLambd  (None, 17, 2)       0           ['tf.math.subtract_18[0][0]',    \n",
      " a)                                                               'tf.broadcast_to_13[0][0]']     \n",
      "                                                                                                  \n",
      " tf.math.subtract_19 (TFOpLambd  (None, 2)           0           ['tf.math.truediv_32[0][0]',     \n",
      " a)                                                               'tf.math.truediv_31[0][0]']     \n",
      "                                                                                                  \n",
      " tf.compat.v1.gather_62 (TFOpLa  (17, 2)             0           ['tf.math.subtract_20[0][0]']    \n",
      " mbda)                                                                                            \n",
      "                                                                                                  \n",
      " tf.compat.v1.norm_12 (TFOpLamb  ()                  0           ['tf.math.subtract_19[0][0]']    \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.compat.v1.norm_13 (TFOpLamb  (2,)                0           ['tf.compat.v1.gather_62[0][0]'] \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.math.multiply_6 (TFOpLambda  ()                  0           ['tf.compat.v1.norm_12[0][0]']   \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.reduce_max_6 (TFOpLamb  ()                  0           ['tf.compat.v1.norm_13[0][0]']   \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.math.maximum_6 (TFOpLambda)  ()                  0           ['tf.math.multiply_6[0][0]',     \n",
      "                                                                  'tf.math.reduce_max_6[0][0]']   \n",
      "                                                                                                  \n",
      " tf.math.truediv_34 (TFOpLambda  (None, 17, 2)       0           ['tf.math.subtract_18[0][0]',    \n",
      " )                                                                'tf.math.maximum_6[0][0]']      \n",
      "                                                                                                  \n",
      " flatten_6 (Flatten)            (None, 34)           0           ['tf.math.truediv_34[0][0]']     \n",
      "                                                                                                  \n",
      " dense_20 (Dense)               (None, 256)          8960        ['flatten_6[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_6 (BatchNo  (None, 256)         1024        ['dense_20[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_14 (Dropout)           (None, 256)          0           ['batch_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " dense_21 (Dense)               (None, 128)          32896       ['dropout_14[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_7 (BatchNo  (None, 128)         512         ['dense_21[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_15 (Dropout)           (None, 128)          0           ['batch_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " dense_22 (Dense)               (None, 64)           8256        ['dropout_15[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_8 (BatchNo  (None, 64)          256         ['dense_22[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_16 (Dropout)           (None, 64)           0           ['batch_normalization_8[0][0]']  \n",
      "                                                                                                  \n",
      " dense_23 (Dense)               (None, 5)            325         ['dropout_16[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 52,229\n",
      "Trainable params: 51,333\n",
      "Non-trainable params: 896\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "inputs = tf.keras.Input(shape= 51)\n",
    "reshaped_inputs = keras.layers.Reshape((17, 3))(inputs)\n",
    "# since the input data will have 17 critical point with (x,y,score) \n",
    "# so we initial the input as 51 entry, and reshapre it to (17,3)\n",
    "embedding = normalize_pose_inputs(reshaped_inputs)\n",
    "outputs = models.DNN(embedding,len(class_names))\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_7\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_8 (InputLayer)           [(None, 51)]         0           []                               \n",
      "                                                                                                  \n",
      " reshape_7 (Reshape)            (None, 17, 3)        0           ['input_8[0][0]']                \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_7 (Sl  (None, 17, 2)       0           ['reshape_7[0][0]']              \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " tf.compat.v1.gather_63 (TFOpLa  (None, 2)           0           ['tf.__operators__.getitem_7[0][0\n",
      " mbda)                                                           ]']                              \n",
      "                                                                                                  \n",
      " tf.compat.v1.gather_64 (TFOpLa  (None, 2)           0           ['tf.__operators__.getitem_7[0][0\n",
      " mbda)                                                           ]']                              \n",
      "                                                                                                  \n",
      " tf.__operators__.add_28 (TFOpL  (None, 2)           0           ['tf.compat.v1.gather_63[0][0]', \n",
      " ambda)                                                           'tf.compat.v1.gather_64[0][0]'] \n",
      "                                                                                                  \n",
      " tf.math.truediv_35 (TFOpLambda  (None, 2)           0           ['tf.__operators__.add_28[0][0]']\n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.compat.v1.size_14 (TFOpLamb  ()                  0           ['tf.__operators__.getitem_7[0][0\n",
      " da)                                                             ]']                              \n",
      "                                                                                                  \n",
      " tf.expand_dims_14 (TFOpLambda)  (None, 1, 2)        0           ['tf.math.truediv_35[0][0]']     \n",
      "                                                                                                  \n",
      " tf.compat.v1.floor_div_14 (TFO  ()                  0           ['tf.compat.v1.size_14[0][0]']   \n",
      " pLambda)                                                                                         \n",
      "                                                                                                  \n",
      " tf.broadcast_to_14 (TFOpLambda  (None, 17, 2)       0           ['tf.expand_dims_14[0][0]',      \n",
      " )                                                                'tf.compat.v1.floor_div_14[0][0]\n",
      "                                                                 ']                               \n",
      "                                                                                                  \n",
      " tf.math.subtract_21 (TFOpLambd  (None, 17, 2)       0           ['tf.__operators__.getitem_7[0][0\n",
      " a)                                                              ]',                              \n",
      "                                                                  'tf.broadcast_to_14[0][0]']     \n",
      "                                                                                                  \n",
      " tf.compat.v1.gather_69 (TFOpLa  (None, 2)           0           ['tf.math.subtract_21[0][0]']    \n",
      " mbda)                                                                                            \n",
      "                                                                                                  \n",
      " tf.compat.v1.gather_70 (TFOpLa  (None, 2)           0           ['tf.math.subtract_21[0][0]']    \n",
      " mbda)                                                                                            \n",
      "                                                                                                  \n",
      " tf.__operators__.add_31 (TFOpL  (None, 2)           0           ['tf.compat.v1.gather_69[0][0]', \n",
      " ambda)                                                           'tf.compat.v1.gather_70[0][0]'] \n",
      "                                                                                                  \n",
      " tf.math.truediv_38 (TFOpLambda  (None, 2)           0           ['tf.__operators__.add_31[0][0]']\n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.compat.v1.size_15 (TFOpLamb  ()                  0           ['tf.math.subtract_21[0][0]']    \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.compat.v1.gather_67 (TFOpLa  (None, 2)           0           ['tf.math.subtract_21[0][0]']    \n",
      " mbda)                                                                                            \n",
      "                                                                                                  \n",
      " tf.compat.v1.gather_68 (TFOpLa  (None, 2)           0           ['tf.math.subtract_21[0][0]']    \n",
      " mbda)                                                                                            \n",
      "                                                                                                  \n",
      " tf.compat.v1.gather_65 (TFOpLa  (None, 2)           0           ['tf.math.subtract_21[0][0]']    \n",
      " mbda)                                                                                            \n",
      "                                                                                                  \n",
      " tf.compat.v1.gather_66 (TFOpLa  (None, 2)           0           ['tf.math.subtract_21[0][0]']    \n",
      " mbda)                                                                                            \n",
      "                                                                                                  \n",
      " tf.expand_dims_15 (TFOpLambda)  (None, 1, 2)        0           ['tf.math.truediv_38[0][0]']     \n",
      "                                                                                                  \n",
      " tf.compat.v1.floor_div_15 (TFO  ()                  0           ['tf.compat.v1.size_15[0][0]']   \n",
      " pLambda)                                                                                         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_30 (TFOpL  (None, 2)           0           ['tf.compat.v1.gather_67[0][0]', \n",
      " ambda)                                                           'tf.compat.v1.gather_68[0][0]'] \n",
      "                                                                                                  \n",
      " tf.__operators__.add_29 (TFOpL  (None, 2)           0           ['tf.compat.v1.gather_65[0][0]', \n",
      " ambda)                                                           'tf.compat.v1.gather_66[0][0]'] \n",
      "                                                                                                  \n",
      " tf.broadcast_to_15 (TFOpLambda  (None, 17, 2)       0           ['tf.expand_dims_15[0][0]',      \n",
      " )                                                                'tf.compat.v1.floor_div_15[0][0]\n",
      "                                                                 ']                               \n",
      "                                                                                                  \n",
      " tf.math.truediv_37 (TFOpLambda  (None, 2)           0           ['tf.__operators__.add_30[0][0]']\n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.truediv_36 (TFOpLambda  (None, 2)           0           ['tf.__operators__.add_29[0][0]']\n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.subtract_23 (TFOpLambd  (None, 17, 2)       0           ['tf.math.subtract_21[0][0]',    \n",
      " a)                                                               'tf.broadcast_to_15[0][0]']     \n",
      "                                                                                                  \n",
      " tf.math.subtract_22 (TFOpLambd  (None, 2)           0           ['tf.math.truediv_37[0][0]',     \n",
      " a)                                                               'tf.math.truediv_36[0][0]']     \n",
      "                                                                                                  \n",
      " tf.compat.v1.gather_71 (TFOpLa  (17, 2)             0           ['tf.math.subtract_23[0][0]']    \n",
      " mbda)                                                                                            \n",
      "                                                                                                  \n",
      " tf.compat.v1.norm_14 (TFOpLamb  ()                  0           ['tf.math.subtract_22[0][0]']    \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.compat.v1.norm_15 (TFOpLamb  (2,)                0           ['tf.compat.v1.gather_71[0][0]'] \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.math.multiply_7 (TFOpLambda  ()                  0           ['tf.compat.v1.norm_14[0][0]']   \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.reduce_max_7 (TFOpLamb  ()                  0           ['tf.compat.v1.norm_15[0][0]']   \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.math.maximum_7 (TFOpLambda)  ()                  0           ['tf.math.multiply_7[0][0]',     \n",
      "                                                                  'tf.math.reduce_max_7[0][0]']   \n",
      "                                                                                                  \n",
      " tf.math.truediv_39 (TFOpLambda  (None, 17, 2)       0           ['tf.math.subtract_21[0][0]',    \n",
      " )                                                                'tf.math.maximum_7[0][0]']      \n",
      "                                                                                                  \n",
      " conv1d_8 (Conv1D)              (None, 15, 64)       448         ['tf.math.truediv_39[0][0]']     \n",
      "                                                                                                  \n",
      " conv1d_9 (Conv1D)              (None, 13, 64)       12352       ['conv1d_8[0][0]']               \n",
      "                                                                                                  \n",
      " max_pooling1d_4 (MaxPooling1D)  (None, 6, 64)       0           ['conv1d_9[0][0]']               \n",
      "                                                                                                  \n",
      " flatten_7 (Flatten)            (None, 384)          0           ['max_pooling1d_4[0][0]']        \n",
      "                                                                                                  \n",
      " dense_24 (Dense)               (None, 128)          49280       ['flatten_7[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_17 (Dropout)           (None, 128)          0           ['dense_24[0][0]']               \n",
      "                                                                                                  \n",
      " dense_25 (Dense)               (None, 64)           8256        ['dropout_17[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_18 (Dropout)           (None, 64)           0           ['dense_25[0][0]']               \n",
      "                                                                                                  \n",
      " dense_26 (Dense)               (None, 5)            325         ['dropout_18[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 70,661\n",
      "Trainable params: 70,661\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "inputs = tf.keras.Input(shape=(51,))\n",
    "\n",
    "# Reshape inputs\n",
    "reshaped_inputs= keras.layers.Reshape((17, 3))(inputs)\n",
    "\n",
    "# Convert landmarks to embedding\n",
    "embedding = normalize_pose_inputs(reshaped_inputs)\n",
    "\n",
    "outputs =models.CNN(embedding,len(class_names))\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_9\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_10 (InputLayer)          [(None, 51)]         0           []                               \n",
      "                                                                                                  \n",
      " reshape_9 (Reshape)            (None, 17, 3)        0           ['input_10[0][0]']               \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_9 (Sl  (None, 17, 2)       0           ['reshape_9[0][0]']              \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " tf.compat.v1.gather_81 (TFOpLa  (None, 2)           0           ['tf.__operators__.getitem_9[0][0\n",
      " mbda)                                                           ]']                              \n",
      "                                                                                                  \n",
      " tf.compat.v1.gather_82 (TFOpLa  (None, 2)           0           ['tf.__operators__.getitem_9[0][0\n",
      " mbda)                                                           ]']                              \n",
      "                                                                                                  \n",
      " tf.__operators__.add_36 (TFOpL  (None, 2)           0           ['tf.compat.v1.gather_81[0][0]', \n",
      " ambda)                                                           'tf.compat.v1.gather_82[0][0]'] \n",
      "                                                                                                  \n",
      " tf.math.truediv_45 (TFOpLambda  (None, 2)           0           ['tf.__operators__.add_36[0][0]']\n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.compat.v1.size_18 (TFOpLamb  ()                  0           ['tf.__operators__.getitem_9[0][0\n",
      " da)                                                             ]']                              \n",
      "                                                                                                  \n",
      " tf.expand_dims_18 (TFOpLambda)  (None, 1, 2)        0           ['tf.math.truediv_45[0][0]']     \n",
      "                                                                                                  \n",
      " tf.compat.v1.floor_div_18 (TFO  ()                  0           ['tf.compat.v1.size_18[0][0]']   \n",
      " pLambda)                                                                                         \n",
      "                                                                                                  \n",
      " tf.broadcast_to_18 (TFOpLambda  (None, 17, 2)       0           ['tf.expand_dims_18[0][0]',      \n",
      " )                                                                'tf.compat.v1.floor_div_18[0][0]\n",
      "                                                                 ']                               \n",
      "                                                                                                  \n",
      " tf.math.subtract_27 (TFOpLambd  (None, 17, 2)       0           ['tf.__operators__.getitem_9[0][0\n",
      " a)                                                              ]',                              \n",
      "                                                                  'tf.broadcast_to_18[0][0]']     \n",
      "                                                                                                  \n",
      " tf.compat.v1.gather_87 (TFOpLa  (None, 2)           0           ['tf.math.subtract_27[0][0]']    \n",
      " mbda)                                                                                            \n",
      "                                                                                                  \n",
      " tf.compat.v1.gather_88 (TFOpLa  (None, 2)           0           ['tf.math.subtract_27[0][0]']    \n",
      " mbda)                                                                                            \n",
      "                                                                                                  \n",
      " tf.__operators__.add_39 (TFOpL  (None, 2)           0           ['tf.compat.v1.gather_87[0][0]', \n",
      " ambda)                                                           'tf.compat.v1.gather_88[0][0]'] \n",
      "                                                                                                  \n",
      " tf.math.truediv_48 (TFOpLambda  (None, 2)           0           ['tf.__operators__.add_39[0][0]']\n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.compat.v1.size_19 (TFOpLamb  ()                  0           ['tf.math.subtract_27[0][0]']    \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.compat.v1.gather_85 (TFOpLa  (None, 2)           0           ['tf.math.subtract_27[0][0]']    \n",
      " mbda)                                                                                            \n",
      "                                                                                                  \n",
      " tf.compat.v1.gather_86 (TFOpLa  (None, 2)           0           ['tf.math.subtract_27[0][0]']    \n",
      " mbda)                                                                                            \n",
      "                                                                                                  \n",
      " tf.compat.v1.gather_83 (TFOpLa  (None, 2)           0           ['tf.math.subtract_27[0][0]']    \n",
      " mbda)                                                                                            \n",
      "                                                                                                  \n",
      " tf.compat.v1.gather_84 (TFOpLa  (None, 2)           0           ['tf.math.subtract_27[0][0]']    \n",
      " mbda)                                                                                            \n",
      "                                                                                                  \n",
      " tf.expand_dims_19 (TFOpLambda)  (None, 1, 2)        0           ['tf.math.truediv_48[0][0]']     \n",
      "                                                                                                  \n",
      " tf.compat.v1.floor_div_19 (TFO  ()                  0           ['tf.compat.v1.size_19[0][0]']   \n",
      " pLambda)                                                                                         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_38 (TFOpL  (None, 2)           0           ['tf.compat.v1.gather_85[0][0]', \n",
      " ambda)                                                           'tf.compat.v1.gather_86[0][0]'] \n",
      "                                                                                                  \n",
      " tf.__operators__.add_37 (TFOpL  (None, 2)           0           ['tf.compat.v1.gather_83[0][0]', \n",
      " ambda)                                                           'tf.compat.v1.gather_84[0][0]'] \n",
      "                                                                                                  \n",
      " tf.broadcast_to_19 (TFOpLambda  (None, 17, 2)       0           ['tf.expand_dims_19[0][0]',      \n",
      " )                                                                'tf.compat.v1.floor_div_19[0][0]\n",
      "                                                                 ']                               \n",
      "                                                                                                  \n",
      " tf.math.truediv_47 (TFOpLambda  (None, 2)           0           ['tf.__operators__.add_38[0][0]']\n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.truediv_46 (TFOpLambda  (None, 2)           0           ['tf.__operators__.add_37[0][0]']\n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.subtract_29 (TFOpLambd  (None, 17, 2)       0           ['tf.math.subtract_27[0][0]',    \n",
      " a)                                                               'tf.broadcast_to_19[0][0]']     \n",
      "                                                                                                  \n",
      " tf.math.subtract_28 (TFOpLambd  (None, 2)           0           ['tf.math.truediv_47[0][0]',     \n",
      " a)                                                               'tf.math.truediv_46[0][0]']     \n",
      "                                                                                                  \n",
      " tf.compat.v1.gather_89 (TFOpLa  (17, 2)             0           ['tf.math.subtract_29[0][0]']    \n",
      " mbda)                                                                                            \n",
      "                                                                                                  \n",
      " tf.compat.v1.norm_18 (TFOpLamb  ()                  0           ['tf.math.subtract_28[0][0]']    \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.compat.v1.norm_19 (TFOpLamb  (2,)                0           ['tf.compat.v1.gather_89[0][0]'] \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.math.multiply_9 (TFOpLambda  ()                  0           ['tf.compat.v1.norm_18[0][0]']   \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.reduce_max_9 (TFOpLamb  ()                  0           ['tf.compat.v1.norm_19[0][0]']   \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.math.maximum_9 (TFOpLambda)  ()                  0           ['tf.math.multiply_9[0][0]',     \n",
      "                                                                  'tf.math.reduce_max_9[0][0]']   \n",
      "                                                                                                  \n",
      " tf.math.truediv_49 (TFOpLambda  (None, 17, 2)       0           ['tf.math.subtract_27[0][0]',    \n",
      " )                                                                'tf.math.maximum_9[0][0]']      \n",
      "                                                                                                  \n",
      " flatten_9 (Flatten)            (None, 34)           0           ['tf.math.truediv_49[0][0]']     \n",
      "                                                                                                  \n",
      " dense_30 (Dense)               (None, 32)           1120        ['flatten_9[0][0]']              \n",
      "                                                                                                  \n",
      " dense_31 (Dense)               (None, 5)            165         ['dense_30[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,285\n",
      "Trainable params: 1,285\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "inputs = tf.keras.Input(shape= 51)\n",
    "reshaped_inputs = keras.layers.Reshape((17, 3))(inputs)\n",
    "# since the input data will have 17 critical point with (x,y,score) \n",
    "# so we initial the input as 51 entry, and reshapre it to (17,3)\n",
    "embedding = normalize_pose_inputs(reshaped_inputs)\n",
    "outputs = models.basic(embedding,len(class_names))\n",
    "\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "1/8 [==>...........................] - ETA: 6s - loss: 1.5684 - accuracy: 0.7500\n",
      "Epoch 1: val_accuracy improved from -inf to 0.36441, saving model to weights.best.hdf5\n",
      "8/8 [==============================] - 2s 105ms/step - loss: 1.5966 - accuracy: 0.3475 - val_loss: 1.5882 - val_accuracy: 0.3644\n",
      "Epoch 2/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 1.6074 - accuracy: 0.3750\n",
      "Epoch 2: val_accuracy improved from 0.36441 to 0.37288, saving model to weights.best.hdf5\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.5846 - accuracy: 0.3729 - val_loss: 1.5806 - val_accuracy: 0.3729\n",
      "Epoch 3/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 1.6154 - accuracy: 0.3125\n",
      "Epoch 3: val_accuracy improved from 0.37288 to 0.39831, saving model to weights.best.hdf5\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.5769 - accuracy: 0.3983 - val_loss: 1.5722 - val_accuracy: 0.3983\n",
      "Epoch 4/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 1.5803 - accuracy: 0.3125\n",
      "Epoch 4: val_accuracy improved from 0.39831 to 0.40678, saving model to weights.best.hdf5\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.5695 - accuracy: 0.3983 - val_loss: 1.5642 - val_accuracy: 0.4068\n",
      "Epoch 5/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 1.5531 - accuracy: 0.3125\n",
      "Epoch 5: val_accuracy did not improve from 0.40678\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 1.5612 - accuracy: 0.4068 - val_loss: 1.5569 - val_accuracy: 0.4068\n",
      "Epoch 6/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 1.5573 - accuracy: 0.5000\n",
      "Epoch 6: val_accuracy improved from 0.40678 to 0.42373, saving model to weights.best.hdf5\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 1.5532 - accuracy: 0.4322 - val_loss: 1.5504 - val_accuracy: 0.4237\n",
      "Epoch 7/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 1.5169 - accuracy: 0.5625\n",
      "Epoch 7: val_accuracy did not improve from 0.42373\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 1.5461 - accuracy: 0.4153 - val_loss: 1.5440 - val_accuracy: 0.3983\n",
      "Epoch 8/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 1.5477 - accuracy: 0.4375\n",
      "Epoch 8: val_accuracy did not improve from 0.42373\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 1.5413 - accuracy: 0.3898 - val_loss: 1.5377 - val_accuracy: 0.3983\n",
      "Epoch 9/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 1.4981 - accuracy: 0.5000\n",
      "Epoch 9: val_accuracy did not improve from 0.42373\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 1.5334 - accuracy: 0.3983 - val_loss: 1.5320 - val_accuracy: 0.3983\n",
      "Epoch 10/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 1.5210 - accuracy: 0.5000\n",
      "Epoch 10: val_accuracy did not improve from 0.42373\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 1.5313 - accuracy: 0.4068 - val_loss: 1.5261 - val_accuracy: 0.4237\n",
      "Epoch 11/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 1.5391 - accuracy: 0.4375\n",
      "Epoch 11: val_accuracy improved from 0.42373 to 0.45763, saving model to weights.best.hdf5\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.5227 - accuracy: 0.4576 - val_loss: 1.5210 - val_accuracy: 0.4576\n",
      "Epoch 12/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 1.6050 - accuracy: 0.3125\n",
      "Epoch 12: val_accuracy did not improve from 0.45763\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 1.5203 - accuracy: 0.4576 - val_loss: 1.5154 - val_accuracy: 0.4407\n",
      "Epoch 13/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 1.4924 - accuracy: 0.5625\n",
      "Epoch 13: val_accuracy improved from 0.45763 to 0.47458, saving model to weights.best.hdf5\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.5145 - accuracy: 0.4746 - val_loss: 1.5102 - val_accuracy: 0.4746\n",
      "Epoch 14/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 1.4406 - accuracy: 0.6250\n",
      "Epoch 14: val_accuracy improved from 0.47458 to 0.48305, saving model to weights.best.hdf5\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.5071 - accuracy: 0.4746 - val_loss: 1.5050 - val_accuracy: 0.4831\n",
      "Epoch 15/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 1.4945 - accuracy: 0.5625\n",
      "Epoch 15: val_accuracy did not improve from 0.48305\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 1.5039 - accuracy: 0.4831 - val_loss: 1.4997 - val_accuracy: 0.4831\n",
      "Epoch 16/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 1.4640 - accuracy: 0.5000\n",
      "Epoch 16: val_accuracy did not improve from 0.48305\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 1.4987 - accuracy: 0.4661 - val_loss: 1.4946 - val_accuracy: 0.4492\n",
      "Epoch 17/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 1.4663 - accuracy: 0.5000\n",
      "Epoch 17: val_accuracy did not improve from 0.48305\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 1.4923 - accuracy: 0.4492 - val_loss: 1.4896 - val_accuracy: 0.4492\n",
      "Epoch 18/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 1.4462 - accuracy: 0.5625\n",
      "Epoch 18: val_accuracy did not improve from 0.48305\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 1.4898 - accuracy: 0.4492 - val_loss: 1.4840 - val_accuracy: 0.4492\n",
      "Epoch 19/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 1.3702 - accuracy: 0.6250\n",
      "Epoch 19: val_accuracy did not improve from 0.48305\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 1.4848 - accuracy: 0.4492 - val_loss: 1.4788 - val_accuracy: 0.4492\n",
      "Epoch 20/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 1.4078 - accuracy: 0.6875\n",
      "Epoch 20: val_accuracy did not improve from 0.48305\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 1.4786 - accuracy: 0.4492 - val_loss: 1.4737 - val_accuracy: 0.4492\n",
      "Epoch 21/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 1.4955 - accuracy: 0.4375\n",
      "Epoch 21: val_accuracy did not improve from 0.48305\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 1.4737 - accuracy: 0.4492 - val_loss: 1.4684 - val_accuracy: 0.4492\n",
      "Epoch 22/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 1.4257 - accuracy: 0.5000\n",
      "Epoch 22: val_accuracy did not improve from 0.48305\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 1.4650 - accuracy: 0.4492 - val_loss: 1.4629 - val_accuracy: 0.4576\n",
      "Epoch 23/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 1.5231 - accuracy: 0.4375\n",
      "Epoch 23: val_accuracy did not improve from 0.48305\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 1.4639 - accuracy: 0.4576 - val_loss: 1.4570 - val_accuracy: 0.4576\n",
      "Epoch 24/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 1.4792 - accuracy: 0.3750\n",
      "Epoch 24: val_accuracy did not improve from 0.48305\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 1.4497 - accuracy: 0.4576 - val_loss: 1.4514 - val_accuracy: 0.4576\n",
      "Epoch 25/200\n",
      "8/8 [==============================] - ETA: 0s - loss: 1.4511 - accuracy: 0.4576\n",
      "Epoch 25: val_accuracy did not improve from 0.48305\n",
      "8/8 [==============================] - 0s 23ms/step - loss: 1.4511 - accuracy: 0.4576 - val_loss: 1.4449 - val_accuracy: 0.4576\n",
      "Epoch 26/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 1.5263 - accuracy: 0.3125\n",
      "Epoch 26: val_accuracy did not improve from 0.48305\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 1.4376 - accuracy: 0.4576 - val_loss: 1.4390 - val_accuracy: 0.4576\n",
      "Epoch 27/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 1.4400 - accuracy: 0.4375\n",
      "Epoch 27: val_accuracy did not improve from 0.48305\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 1.4343 - accuracy: 0.4576 - val_loss: 1.4322 - val_accuracy: 0.4576\n",
      "Epoch 28/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 1.4679 - accuracy: 0.4375\n",
      "Epoch 28: val_accuracy did not improve from 0.48305\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 1.4289 - accuracy: 0.4576 - val_loss: 1.4257 - val_accuracy: 0.4576\n",
      "Epoch 29/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 1.3510 - accuracy: 0.6250\n",
      "Epoch 29: val_accuracy did not improve from 0.48305\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 1.4195 - accuracy: 0.4661 - val_loss: 1.4193 - val_accuracy: 0.4661\n",
      "Epoch 30/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 1.3408 - accuracy: 0.5625\n",
      "Epoch 30: val_accuracy did not improve from 0.48305\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 1.4177 - accuracy: 0.4576 - val_loss: 1.4125 - val_accuracy: 0.4746\n",
      "Epoch 31/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 1.4797 - accuracy: 0.3750\n",
      "Epoch 31: val_accuracy did not improve from 0.48305\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 1.4092 - accuracy: 0.4831 - val_loss: 1.4057 - val_accuracy: 0.4831\n",
      "Epoch 32/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 1.4126 - accuracy: 0.4375\n",
      "Epoch 32: val_accuracy improved from 0.48305 to 0.50847, saving model to weights.best.hdf5\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.4017 - accuracy: 0.5085 - val_loss: 1.3989 - val_accuracy: 0.5085\n",
      "Epoch 33/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 1.4386 - accuracy: 0.5000\n",
      "Epoch 33: val_accuracy improved from 0.50847 to 0.54237, saving model to weights.best.hdf5\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.3931 - accuracy: 0.5254 - val_loss: 1.3921 - val_accuracy: 0.5424\n",
      "Epoch 34/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 1.5214 - accuracy: 0.2500\n",
      "Epoch 34: val_accuracy improved from 0.54237 to 0.55085, saving model to weights.best.hdf5\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.3867 - accuracy: 0.5424 - val_loss: 1.3853 - val_accuracy: 0.5508\n",
      "Epoch 35/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 1.4128 - accuracy: 0.5000\n",
      "Epoch 35: val_accuracy did not improve from 0.55085\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 1.3776 - accuracy: 0.5508 - val_loss: 1.3780 - val_accuracy: 0.5508\n",
      "Epoch 36/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 1.4769 - accuracy: 0.4375\n",
      "Epoch 36: val_accuracy did not improve from 0.55085\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 1.3651 - accuracy: 0.5508 - val_loss: 1.3711 - val_accuracy: 0.5508\n",
      "Epoch 37/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 1.4214 - accuracy: 0.5000\n",
      "Epoch 37: val_accuracy improved from 0.55085 to 0.55932, saving model to weights.best.hdf5\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.3667 - accuracy: 0.5424 - val_loss: 1.3637 - val_accuracy: 0.5593\n",
      "Epoch 38/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 1.4781 - accuracy: 0.3750\n",
      "Epoch 38: val_accuracy did not improve from 0.55932\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 1.3546 - accuracy: 0.5508 - val_loss: 1.3569 - val_accuracy: 0.5593\n",
      "Epoch 39/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 1.3143 - accuracy: 0.6875\n",
      "Epoch 39: val_accuracy improved from 0.55932 to 0.56780, saving model to weights.best.hdf5\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.3523 - accuracy: 0.5424 - val_loss: 1.3494 - val_accuracy: 0.5678\n",
      "Epoch 40/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 1.3067 - accuracy: 0.5625\n",
      "Epoch 40: val_accuracy did not improve from 0.56780\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 1.3429 - accuracy: 0.5339 - val_loss: 1.3416 - val_accuracy: 0.5593\n",
      "Epoch 41/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 1.2687 - accuracy: 0.6875\n",
      "Epoch 41: val_accuracy did not improve from 0.56780\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 1.3347 - accuracy: 0.5593 - val_loss: 1.3345 - val_accuracy: 0.5678\n",
      "Epoch 42/200\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 1.3347 - accuracy: 0.5625\n",
      "Epoch 42: val_accuracy did not improve from 0.56780\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.3236 - accuracy: 0.5678 - val_loss: 1.3269 - val_accuracy: 0.5678\n",
      "Epoch 43/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 1.3560 - accuracy: 0.4375\n",
      "Epoch 43: val_accuracy did not improve from 0.56780\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 1.3239 - accuracy: 0.5593 - val_loss: 1.3190 - val_accuracy: 0.5678\n",
      "Epoch 44/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 1.2814 - accuracy: 0.5625\n",
      "Epoch 44: val_accuracy improved from 0.56780 to 0.57627, saving model to weights.best.hdf5\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.3105 - accuracy: 0.5678 - val_loss: 1.3113 - val_accuracy: 0.5763\n",
      "Epoch 45/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 1.1017 - accuracy: 0.7500\n",
      "Epoch 45: val_accuracy did not improve from 0.57627\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 1.2983 - accuracy: 0.5678 - val_loss: 1.3032 - val_accuracy: 0.5763\n",
      "Epoch 46/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 1.3225 - accuracy: 0.6250\n",
      "Epoch 46: val_accuracy did not improve from 0.57627\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 1.3003 - accuracy: 0.5678 - val_loss: 1.2946 - val_accuracy: 0.5763\n",
      "Epoch 47/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 1.2348 - accuracy: 0.5625\n",
      "Epoch 47: val_accuracy did not improve from 0.57627\n",
      "8/8 [==============================] - 0s 23ms/step - loss: 1.2859 - accuracy: 0.5678 - val_loss: 1.2868 - val_accuracy: 0.5763\n",
      "Epoch 48/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 1.3478 - accuracy: 0.5000\n",
      "Epoch 48: val_accuracy improved from 0.57627 to 0.59322, saving model to weights.best.hdf5\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.2849 - accuracy: 0.5678 - val_loss: 1.2791 - val_accuracy: 0.5932\n",
      "Epoch 49/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 1.3137 - accuracy: 0.5000\n",
      "Epoch 49: val_accuracy improved from 0.59322 to 0.60169, saving model to weights.best.hdf5\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.2697 - accuracy: 0.5932 - val_loss: 1.2714 - val_accuracy: 0.6017\n",
      "Epoch 50/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 1.2406 - accuracy: 0.6250\n",
      "Epoch 50: val_accuracy did not improve from 0.60169\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 1.2610 - accuracy: 0.6017 - val_loss: 1.2639 - val_accuracy: 0.6017\n",
      "Epoch 51/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 1.2874 - accuracy: 0.5625\n",
      "Epoch 51: val_accuracy did not improve from 0.60169\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 1.2519 - accuracy: 0.5932 - val_loss: 1.2558 - val_accuracy: 0.6017\n",
      "Epoch 52/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 1.2672 - accuracy: 0.5625\n",
      "Epoch 52: val_accuracy did not improve from 0.60169\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 1.2549 - accuracy: 0.6017 - val_loss: 1.2477 - val_accuracy: 0.6017\n",
      "Epoch 53/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 1.1691 - accuracy: 0.6875\n",
      "Epoch 53: val_accuracy did not improve from 0.60169\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 1.2393 - accuracy: 0.5932 - val_loss: 1.2401 - val_accuracy: 0.6017\n",
      "Epoch 54/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 1.2504 - accuracy: 0.5625\n",
      "Epoch 54: val_accuracy did not improve from 0.60169\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 1.2299 - accuracy: 0.6017 - val_loss: 1.2320 - val_accuracy: 0.6017\n",
      "Epoch 55/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 1.0810 - accuracy: 0.7500\n",
      "Epoch 55: val_accuracy improved from 0.60169 to 0.61864, saving model to weights.best.hdf5\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.2229 - accuracy: 0.6017 - val_loss: 1.2238 - val_accuracy: 0.6186\n",
      "Epoch 56/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 1.3156 - accuracy: 0.6250\n",
      "Epoch 56: val_accuracy did not improve from 0.61864\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 1.2190 - accuracy: 0.6186 - val_loss: 1.2159 - val_accuracy: 0.6186\n",
      "Epoch 57/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 1.2286 - accuracy: 0.5625\n",
      "Epoch 57: val_accuracy did not improve from 0.61864\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 1.2061 - accuracy: 0.6186 - val_loss: 1.2080 - val_accuracy: 0.6186\n",
      "Epoch 58/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 1.1836 - accuracy: 0.6250\n",
      "Epoch 58: val_accuracy did not improve from 0.61864\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 1.2025 - accuracy: 0.6102 - val_loss: 1.2002 - val_accuracy: 0.6186\n",
      "Epoch 59/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 1.2118 - accuracy: 0.6250\n",
      "Epoch 59: val_accuracy improved from 0.61864 to 0.62712, saving model to weights.best.hdf5\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.1921 - accuracy: 0.6102 - val_loss: 1.1924 - val_accuracy: 0.6271\n",
      "Epoch 60/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 1.0351 - accuracy: 0.7500\n",
      "Epoch 60: val_accuracy improved from 0.62712 to 0.63559, saving model to weights.best.hdf5\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.1850 - accuracy: 0.6271 - val_loss: 1.1845 - val_accuracy: 0.6356\n",
      "Epoch 61/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 1.1491 - accuracy: 0.7500\n",
      "Epoch 61: val_accuracy did not improve from 0.63559\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 1.1777 - accuracy: 0.6356 - val_loss: 1.1762 - val_accuracy: 0.6356\n",
      "Epoch 62/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 1.2044 - accuracy: 0.5000\n",
      "Epoch 62: val_accuracy improved from 0.63559 to 0.65254, saving model to weights.best.hdf5\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.1701 - accuracy: 0.6271 - val_loss: 1.1684 - val_accuracy: 0.6525\n",
      "Epoch 63/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 1.0181 - accuracy: 0.8125\n",
      "Epoch 63: val_accuracy did not improve from 0.65254\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 1.1573 - accuracy: 0.6441 - val_loss: 1.1605 - val_accuracy: 0.6525\n",
      "Epoch 64/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.9742 - accuracy: 0.8125\n",
      "Epoch 64: val_accuracy did not improve from 0.65254\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 1.1518 - accuracy: 0.6441 - val_loss: 1.1523 - val_accuracy: 0.6525\n",
      "Epoch 65/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 1.1034 - accuracy: 0.6250\n",
      "Epoch 65: val_accuracy did not improve from 0.65254\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 1.1473 - accuracy: 0.6441 - val_loss: 1.1443 - val_accuracy: 0.6525\n",
      "Epoch 66/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 1.1608 - accuracy: 0.5625\n",
      "Epoch 66: val_accuracy did not improve from 0.65254\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 1.1417 - accuracy: 0.6441 - val_loss: 1.1366 - val_accuracy: 0.6525\n",
      "Epoch 67/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 1.0005 - accuracy: 0.8125\n",
      "Epoch 67: val_accuracy did not improve from 0.65254\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 1.1264 - accuracy: 0.6441 - val_loss: 1.1293 - val_accuracy: 0.6441\n",
      "Epoch 68/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 1.0987 - accuracy: 0.6875\n",
      "Epoch 68: val_accuracy did not improve from 0.65254\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 1.1203 - accuracy: 0.6441 - val_loss: 1.1223 - val_accuracy: 0.6525\n",
      "Epoch 69/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 1.1596 - accuracy: 0.6250\n",
      "Epoch 69: val_accuracy did not improve from 0.65254\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 1.1200 - accuracy: 0.6525 - val_loss: 1.1155 - val_accuracy: 0.6525\n",
      "Epoch 70/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 1.1608 - accuracy: 0.6875\n",
      "Epoch 70: val_accuracy did not improve from 0.65254\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 1.1086 - accuracy: 0.6441 - val_loss: 1.1086 - val_accuracy: 0.6525\n",
      "Epoch 71/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 1.0740 - accuracy: 0.6250\n",
      "Epoch 71: val_accuracy did not improve from 0.65254\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 1.0968 - accuracy: 0.6356 - val_loss: 1.1012 - val_accuracy: 0.6525\n",
      "Epoch 72/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 1.1480 - accuracy: 0.6250\n",
      "Epoch 72: val_accuracy did not improve from 0.65254\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 1.0937 - accuracy: 0.6525 - val_loss: 1.0937 - val_accuracy: 0.6525\n",
      "Epoch 73/200\n",
      "8/8 [==============================] - ETA: 0s - loss: 1.0859 - accuracy: 0.6441\n",
      "Epoch 73: val_accuracy did not improve from 0.65254\n",
      "8/8 [==============================] - 0s 24ms/step - loss: 1.0859 - accuracy: 0.6441 - val_loss: 1.0865 - val_accuracy: 0.6441\n",
      "Epoch 74/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 1.1091 - accuracy: 0.6250\n",
      "Epoch 74: val_accuracy improved from 0.65254 to 0.66102, saving model to weights.best.hdf5\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.0804 - accuracy: 0.6441 - val_loss: 1.0788 - val_accuracy: 0.6610\n",
      "Epoch 75/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.9601 - accuracy: 0.7500\n",
      "Epoch 75: val_accuracy did not improve from 0.66102\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 1.0699 - accuracy: 0.6441 - val_loss: 1.0720 - val_accuracy: 0.6525\n",
      "Epoch 76/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 1.1996 - accuracy: 0.5625\n",
      "Epoch 76: val_accuracy did not improve from 0.66102\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 1.0631 - accuracy: 0.6610 - val_loss: 1.0648 - val_accuracy: 0.6610\n",
      "Epoch 77/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 1.0028 - accuracy: 0.6875\n",
      "Epoch 77: val_accuracy did not improve from 0.66102\n",
      "8/8 [==============================] - 0s 23ms/step - loss: 1.0658 - accuracy: 0.6610 - val_loss: 1.0579 - val_accuracy: 0.6525\n",
      "Epoch 78/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 1.0942 - accuracy: 0.5000\n",
      "Epoch 78: val_accuracy improved from 0.66102 to 0.69492, saving model to weights.best.hdf5\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.0517 - accuracy: 0.6780 - val_loss: 1.0509 - val_accuracy: 0.6949\n",
      "Epoch 79/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 1.0704 - accuracy: 0.6250\n",
      "Epoch 79: val_accuracy improved from 0.69492 to 0.70339, saving model to weights.best.hdf5\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.0465 - accuracy: 0.7034 - val_loss: 1.0436 - val_accuracy: 0.7034\n",
      "Epoch 80/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 1.0137 - accuracy: 0.7500\n",
      "Epoch 80: val_accuracy did not improve from 0.70339\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 1.0379 - accuracy: 0.7034 - val_loss: 1.0372 - val_accuracy: 0.7034\n",
      "Epoch 81/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 1.0032 - accuracy: 0.8125\n",
      "Epoch 81: val_accuracy did not improve from 0.70339\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 1.0353 - accuracy: 0.7034 - val_loss: 1.0308 - val_accuracy: 0.7034\n",
      "Epoch 82/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 1.0757 - accuracy: 0.6250\n",
      "Epoch 82: val_accuracy did not improve from 0.70339\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 1.0237 - accuracy: 0.6949 - val_loss: 1.0248 - val_accuracy: 0.7034\n",
      "Epoch 83/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 1.0162 - accuracy: 0.8125\n",
      "Epoch 83: val_accuracy did not improve from 0.70339\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 1.0231 - accuracy: 0.7034 - val_loss: 1.0182 - val_accuracy: 0.7034\n",
      "Epoch 84/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 1.0397 - accuracy: 0.7500\n",
      "Epoch 84: val_accuracy improved from 0.70339 to 0.71186, saving model to weights.best.hdf5\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.0129 - accuracy: 0.7119 - val_loss: 1.0114 - val_accuracy: 0.7119\n",
      "Epoch 85/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.8827 - accuracy: 0.8125\n",
      "Epoch 85: val_accuracy did not improve from 0.71186\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 1.0026 - accuracy: 0.7119 - val_loss: 1.0047 - val_accuracy: 0.7119\n",
      "Epoch 86/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 1.0904 - accuracy: 0.6250\n",
      "Epoch 86: val_accuracy did not improve from 0.71186\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.9985 - accuracy: 0.7203 - val_loss: 0.9982 - val_accuracy: 0.7119\n",
      "Epoch 87/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 1.0305 - accuracy: 0.6875\n",
      "Epoch 87: val_accuracy improved from 0.71186 to 0.72034, saving model to weights.best.hdf5\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 0.9908 - accuracy: 0.7203 - val_loss: 0.9920 - val_accuracy: 0.7203\n",
      "Epoch 88/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 1.1061 - accuracy: 0.6875\n",
      "Epoch 88: val_accuracy did not improve from 0.72034\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.9825 - accuracy: 0.7203 - val_loss: 0.9859 - val_accuracy: 0.7203\n",
      "Epoch 89/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 1.1418 - accuracy: 0.5000\n",
      "Epoch 89: val_accuracy did not improve from 0.72034\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.9891 - accuracy: 0.7203 - val_loss: 0.9798 - val_accuracy: 0.7203\n",
      "Epoch 90/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 1.0138 - accuracy: 0.7500\n",
      "Epoch 90: val_accuracy did not improve from 0.72034\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.9732 - accuracy: 0.7119 - val_loss: 0.9741 - val_accuracy: 0.7203\n",
      "Epoch 91/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.8909 - accuracy: 0.8750\n",
      "Epoch 91: val_accuracy did not improve from 0.72034\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.9737 - accuracy: 0.7203 - val_loss: 0.9677 - val_accuracy: 0.7203\n",
      "Epoch 92/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.8399 - accuracy: 0.8125\n",
      "Epoch 92: val_accuracy did not improve from 0.72034\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.9640 - accuracy: 0.7203 - val_loss: 0.9621 - val_accuracy: 0.7203\n",
      "Epoch 93/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.8066 - accuracy: 0.8125\n",
      "Epoch 93: val_accuracy did not improve from 0.72034\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.9569 - accuracy: 0.7119 - val_loss: 0.9562 - val_accuracy: 0.7203\n",
      "Epoch 94/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.6952 - accuracy: 0.8750\n",
      "Epoch 94: val_accuracy did not improve from 0.72034\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.9474 - accuracy: 0.7203 - val_loss: 0.9501 - val_accuracy: 0.7203\n",
      "Epoch 95/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.8317 - accuracy: 0.8750\n",
      "Epoch 95: val_accuracy did not improve from 0.72034\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.9420 - accuracy: 0.7119 - val_loss: 0.9444 - val_accuracy: 0.7034\n",
      "Epoch 96/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.9622 - accuracy: 0.6250\n",
      "Epoch 96: val_accuracy did not improve from 0.72034\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.9325 - accuracy: 0.7034 - val_loss: 0.9379 - val_accuracy: 0.7119\n",
      "Epoch 97/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.9310 - accuracy: 0.6875\n",
      "Epoch 97: val_accuracy did not improve from 0.72034\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.9364 - accuracy: 0.7034 - val_loss: 0.9316 - val_accuracy: 0.7203\n",
      "Epoch 98/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.9722 - accuracy: 0.6250\n",
      "Epoch 98: val_accuracy improved from 0.72034 to 0.72881, saving model to weights.best.hdf5\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 0.9261 - accuracy: 0.7119 - val_loss: 0.9258 - val_accuracy: 0.7288\n",
      "Epoch 99/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.7706 - accuracy: 0.8750\n",
      "Epoch 99: val_accuracy did not improve from 0.72881\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.9270 - accuracy: 0.7203 - val_loss: 0.9202 - val_accuracy: 0.7288\n",
      "Epoch 100/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.9226 - accuracy: 0.6875\n",
      "Epoch 100: val_accuracy did not improve from 0.72881\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.9156 - accuracy: 0.7288 - val_loss: 0.9147 - val_accuracy: 0.7288\n",
      "Epoch 101/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.8966 - accuracy: 0.8125\n",
      "Epoch 101: val_accuracy improved from 0.72881 to 0.74576, saving model to weights.best.hdf5\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 0.9068 - accuracy: 0.7119 - val_loss: 0.9095 - val_accuracy: 0.7458\n",
      "Epoch 102/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 1.1548 - accuracy: 0.6250\n",
      "Epoch 102: val_accuracy did not improve from 0.74576\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.9042 - accuracy: 0.7288 - val_loss: 0.9044 - val_accuracy: 0.7458\n",
      "Epoch 103/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.9343 - accuracy: 0.7500\n",
      "Epoch 103: val_accuracy did not improve from 0.74576\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.9047 - accuracy: 0.7373 - val_loss: 0.8990 - val_accuracy: 0.7458\n",
      "Epoch 104/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.9691 - accuracy: 0.6250\n",
      "Epoch 104: val_accuracy did not improve from 0.74576\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.8928 - accuracy: 0.7288 - val_loss: 0.8941 - val_accuracy: 0.7458\n",
      "Epoch 105/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 1.0371 - accuracy: 0.7500\n",
      "Epoch 105: val_accuracy improved from 0.74576 to 0.75424, saving model to weights.best.hdf5\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 0.8875 - accuracy: 0.7373 - val_loss: 0.8894 - val_accuracy: 0.7542\n",
      "Epoch 106/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 1.0959 - accuracy: 0.6875\n",
      "Epoch 106: val_accuracy improved from 0.75424 to 0.76271, saving model to weights.best.hdf5\n",
      "8/8 [==============================] - 0s 39ms/step - loss: 0.8868 - accuracy: 0.7542 - val_loss: 0.8846 - val_accuracy: 0.7627\n",
      "Epoch 107/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 1.0483 - accuracy: 0.6250\n",
      "Epoch 107: val_accuracy did not improve from 0.76271\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.8767 - accuracy: 0.7373 - val_loss: 0.8785 - val_accuracy: 0.7627\n",
      "Epoch 108/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.9631 - accuracy: 0.8125\n",
      "Epoch 108: val_accuracy did not improve from 0.76271\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 0.8731 - accuracy: 0.7373 - val_loss: 0.8734 - val_accuracy: 0.7627\n",
      "Epoch 109/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 1.0376 - accuracy: 0.5625\n",
      "Epoch 109: val_accuracy did not improve from 0.76271\n",
      "8/8 [==============================] - 0s 23ms/step - loss: 0.8700 - accuracy: 0.7373 - val_loss: 0.8680 - val_accuracy: 0.7542\n",
      "Epoch 110/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.7958 - accuracy: 0.6875\n",
      "Epoch 110: val_accuracy did not improve from 0.76271\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.8631 - accuracy: 0.7288 - val_loss: 0.8631 - val_accuracy: 0.7542\n",
      "Epoch 111/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.8717 - accuracy: 0.7500\n",
      "Epoch 111: val_accuracy did not improve from 0.76271\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.8641 - accuracy: 0.7373 - val_loss: 0.8586 - val_accuracy: 0.7542\n",
      "Epoch 112/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.8844 - accuracy: 0.6250\n",
      "Epoch 112: val_accuracy did not improve from 0.76271\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.8767 - accuracy: 0.7119 - val_loss: 0.8543 - val_accuracy: 0.7542\n",
      "Epoch 113/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.7733 - accuracy: 0.8125\n",
      "Epoch 113: val_accuracy did not improve from 0.76271\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.8485 - accuracy: 0.7203 - val_loss: 0.8510 - val_accuracy: 0.7458\n",
      "Epoch 114/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.8982 - accuracy: 0.6875\n",
      "Epoch 114: val_accuracy did not improve from 0.76271\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.8472 - accuracy: 0.7288 - val_loss: 0.8465 - val_accuracy: 0.7373\n",
      "Epoch 115/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.7856 - accuracy: 0.8125\n",
      "Epoch 115: val_accuracy did not improve from 0.76271\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.8362 - accuracy: 0.7373 - val_loss: 0.8415 - val_accuracy: 0.7542\n",
      "Epoch 116/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.6817 - accuracy: 0.8125\n",
      "Epoch 116: val_accuracy did not improve from 0.76271\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.8383 - accuracy: 0.7203 - val_loss: 0.8358 - val_accuracy: 0.7627\n",
      "Epoch 117/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.8614 - accuracy: 0.6875\n",
      "Epoch 117: val_accuracy did not improve from 0.76271\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.8335 - accuracy: 0.7373 - val_loss: 0.8311 - val_accuracy: 0.7627\n",
      "Epoch 118/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 1.0359 - accuracy: 0.6875\n",
      "Epoch 118: val_accuracy did not improve from 0.76271\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.8319 - accuracy: 0.7458 - val_loss: 0.8267 - val_accuracy: 0.7627\n",
      "Epoch 119/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.7378 - accuracy: 0.8125\n",
      "Epoch 119: val_accuracy improved from 0.76271 to 0.77119, saving model to weights.best.hdf5\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 0.8258 - accuracy: 0.7542 - val_loss: 0.8226 - val_accuracy: 0.7712\n",
      "Epoch 120/200\n",
      "6/8 [=====================>........] - ETA: 0s - loss: 0.8424 - accuracy: 0.7292\n",
      "Epoch 120: val_accuracy did not improve from 0.77119\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 0.8142 - accuracy: 0.7458 - val_loss: 0.8187 - val_accuracy: 0.7712\n",
      "Epoch 121/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.7994 - accuracy: 0.7500\n",
      "Epoch 121: val_accuracy did not improve from 0.77119\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.8132 - accuracy: 0.7627 - val_loss: 0.8144 - val_accuracy: 0.7627\n",
      "Epoch 122/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 1.0933 - accuracy: 0.6250\n",
      "Epoch 122: val_accuracy did not improve from 0.77119\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.8080 - accuracy: 0.7542 - val_loss: 0.8101 - val_accuracy: 0.7712\n",
      "Epoch 123/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.8009 - accuracy: 0.6875\n",
      "Epoch 123: val_accuracy did not improve from 0.77119\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.8092 - accuracy: 0.7712 - val_loss: 0.8059 - val_accuracy: 0.7712\n",
      "Epoch 124/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.6784 - accuracy: 0.7500\n",
      "Epoch 124: val_accuracy did not improve from 0.77119\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.8086 - accuracy: 0.7627 - val_loss: 0.8020 - val_accuracy: 0.7627\n",
      "Epoch 125/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.6697 - accuracy: 0.8750\n",
      "Epoch 125: val_accuracy did not improve from 0.77119\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.8064 - accuracy: 0.7542 - val_loss: 0.7986 - val_accuracy: 0.7712\n",
      "Epoch 126/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.7592 - accuracy: 0.8750\n",
      "Epoch 126: val_accuracy did not improve from 0.77119\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.8019 - accuracy: 0.7712 - val_loss: 0.7947 - val_accuracy: 0.7712\n",
      "Epoch 127/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.9084 - accuracy: 0.6875\n",
      "Epoch 127: val_accuracy did not improve from 0.77119\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.7885 - accuracy: 0.7627 - val_loss: 0.7907 - val_accuracy: 0.7712\n",
      "Epoch 128/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.6578 - accuracy: 0.8125\n",
      "Epoch 128: val_accuracy improved from 0.77119 to 0.77966, saving model to weights.best.hdf5\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 0.7858 - accuracy: 0.7542 - val_loss: 0.7866 - val_accuracy: 0.7797\n",
      "Epoch 129/200\n",
      "5/8 [=================>............] - ETA: 0s - loss: 0.7811 - accuracy: 0.7750\n",
      "Epoch 129: val_accuracy did not improve from 0.77966\n",
      "8/8 [==============================] - 0s 22ms/step - loss: 0.7988 - accuracy: 0.7542 - val_loss: 0.7826 - val_accuracy: 0.7712\n",
      "Epoch 130/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.7450 - accuracy: 0.7500\n",
      "Epoch 130: val_accuracy did not improve from 0.77966\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.7856 - accuracy: 0.7627 - val_loss: 0.7793 - val_accuracy: 0.7797\n",
      "Epoch 131/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.7419 - accuracy: 0.8125\n",
      "Epoch 131: val_accuracy did not improve from 0.77966\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.7785 - accuracy: 0.7627 - val_loss: 0.7763 - val_accuracy: 0.7797\n",
      "Epoch 132/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.7423 - accuracy: 0.7500\n",
      "Epoch 132: val_accuracy did not improve from 0.77966\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.7796 - accuracy: 0.7542 - val_loss: 0.7727 - val_accuracy: 0.7797\n",
      "Epoch 133/200\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.7687 - accuracy: 0.7542\n",
      "Epoch 133: val_accuracy did not improve from 0.77966\n",
      "8/8 [==============================] - 0s 22ms/step - loss: 0.7687 - accuracy: 0.7542 - val_loss: 0.7693 - val_accuracy: 0.7797\n",
      "Epoch 134/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 1.0344 - accuracy: 0.4375\n",
      "Epoch 134: val_accuracy did not improve from 0.77966\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.7624 - accuracy: 0.7458 - val_loss: 0.7652 - val_accuracy: 0.7797\n",
      "Epoch 135/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.6458 - accuracy: 0.8125\n",
      "Epoch 135: val_accuracy did not improve from 0.77966\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.7600 - accuracy: 0.7627 - val_loss: 0.7611 - val_accuracy: 0.7797\n",
      "Epoch 136/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 1.0264 - accuracy: 0.5625\n",
      "Epoch 136: val_accuracy did not improve from 0.77966\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.7630 - accuracy: 0.7712 - val_loss: 0.7574 - val_accuracy: 0.7797\n",
      "Epoch 137/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.5873 - accuracy: 0.8750\n",
      "Epoch 137: val_accuracy did not improve from 0.77966\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.7553 - accuracy: 0.7797 - val_loss: 0.7540 - val_accuracy: 0.7797\n",
      "Epoch 138/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.9350 - accuracy: 0.7500\n",
      "Epoch 138: val_accuracy did not improve from 0.77966\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.7523 - accuracy: 0.7712 - val_loss: 0.7509 - val_accuracy: 0.7797\n",
      "Epoch 139/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.6313 - accuracy: 0.7500\n",
      "Epoch 139: val_accuracy did not improve from 0.77966\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.7492 - accuracy: 0.7458 - val_loss: 0.7477 - val_accuracy: 0.7797\n",
      "Epoch 140/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.7158 - accuracy: 0.6250\n",
      "Epoch 140: val_accuracy did not improve from 0.77966\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.7450 - accuracy: 0.7458 - val_loss: 0.7440 - val_accuracy: 0.7627\n",
      "Epoch 141/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.6692 - accuracy: 0.7500\n",
      "Epoch 141: val_accuracy did not improve from 0.77966\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.7551 - accuracy: 0.7627 - val_loss: 0.7407 - val_accuracy: 0.7627\n",
      "Epoch 142/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.7406 - accuracy: 0.6250\n",
      "Epoch 142: val_accuracy did not improve from 0.77966\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.7466 - accuracy: 0.7373 - val_loss: 0.7379 - val_accuracy: 0.7712\n",
      "Epoch 143/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.9629 - accuracy: 0.7500\n",
      "Epoch 143: val_accuracy did not improve from 0.77966\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.7349 - accuracy: 0.7458 - val_loss: 0.7349 - val_accuracy: 0.7797\n",
      "Epoch 144/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.6928 - accuracy: 0.8125\n",
      "Epoch 144: val_accuracy did not improve from 0.77966\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.7378 - accuracy: 0.7542 - val_loss: 0.7313 - val_accuracy: 0.7712\n",
      "Epoch 145/200\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.7289 - accuracy: 0.7627\n",
      "Epoch 145: val_accuracy did not improve from 0.77966\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 0.7289 - accuracy: 0.7627 - val_loss: 0.7279 - val_accuracy: 0.7712\n",
      "Epoch 146/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.7438 - accuracy: 0.7500\n",
      "Epoch 146: val_accuracy did not improve from 0.77966\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.7231 - accuracy: 0.7627 - val_loss: 0.7249 - val_accuracy: 0.7712\n",
      "Epoch 147/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.7982 - accuracy: 0.6875\n",
      "Epoch 147: val_accuracy did not improve from 0.77966\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.7251 - accuracy: 0.7712 - val_loss: 0.7218 - val_accuracy: 0.7712\n",
      "Epoch 148/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.5554 - accuracy: 0.8750\n",
      "Epoch 148: val_accuracy did not improve from 0.77966\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.7364 - accuracy: 0.7712 - val_loss: 0.7187 - val_accuracy: 0.7797\n"
     ]
    }
   ],
   "source": [
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Add a checkpoint callback to store the checkpoint that has the highest\n",
    "# validation accuracy.\n",
    "checkpoint_path = \"weights.best.hdf5\"\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(checkpoint_path,\n",
    "                             monitor='val_accuracy',\n",
    "                             verbose=1,\n",
    "                             save_best_only=True,\n",
    "                             mode='max')\n",
    "earlystopping = keras.callbacks.EarlyStopping(monitor='val_accuracy', \n",
    "                                              patience=20)\n",
    "\n",
    "# Start training\n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=200,\n",
    "                    batch_size=16,\n",
    "                    validation_data=(x_test, y_test),\n",
    "                    callbacks=[checkpoint, earlystopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "# Visualize the training history to see whether you're overfitting.\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['TRAIN', 'VAL'], loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 4ms/step\n",
      "Confusion matrix, without normalization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\LA_lab\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\User\\anaconda3\\envs\\LA_lab\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\User\\anaconda3\\envs\\LA_lab\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   cross_leg       0.67      0.80      0.73        20\n",
      "   cross_one       0.00      0.00      0.00        17\n",
      "       hunch       0.85      0.92      0.88        25\n",
      "         lie       0.92      0.92      0.92        25\n",
      "    straight       0.64      0.87      0.74        31\n",
      "\n",
      "    accuracy                           0.75       118\n",
      "   macro avg       0.62      0.70      0.65       118\n",
      "weighted avg       0.66      0.75      0.70       118\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "  \"\"\"Plots the confusion matrix.\"\"\"\n",
    "  if normalize:\n",
    "    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    print(\"Normalized confusion matrix\")\n",
    "  else:\n",
    "    print('Confusion matrix, without normalization')\n",
    "\n",
    "  plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "  plt.title(title)\n",
    "  plt.colorbar()\n",
    "  tick_marks = np.arange(len(classes))\n",
    "  plt.xticks(tick_marks, classes, rotation=55)\n",
    "  plt.yticks(tick_marks, classes)\n",
    "  fmt = '.2f' if normalize else 'd'\n",
    "  thresh = cm.max() / 2.\n",
    "  for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "    plt.text(j, i, format(cm[i, j], fmt),\n",
    "              horizontalalignment=\"center\",\n",
    "              color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "  plt.ylabel('True label')\n",
    "  plt.xlabel('Predicted label')\n",
    "  plt.tight_layout()\n",
    "\n",
    "# Classify pose in the TEST dataset using the trained model\n",
    "y_pred = model.predict(x_test)\n",
    "\n",
    "# Convert the prediction result to class name\n",
    "y_pred_label = [class_names[i] for i in np.argmax(y_pred, axis=1)]\n",
    "y_true_label = [class_names[i] for i in np.argmax(y_test, axis=1)]\n",
    "\n",
    "# Plot the confusion matrix\n",
    "cm = confusion_matrix(np.argmax(y_test, axis=1), np.argmax(y_pred, axis=1))\n",
    "plot_confusion_matrix(cm,\n",
    "                      class_names,\n",
    "                      title ='Confusion Matrix of Pose Classification Model')\n",
    "\n",
    "# Print the classification report\n",
    "print('\\nClassification Report:\\n', classification_report(y_true_label,\n",
    "                                                          y_pred_label))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LA_lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
